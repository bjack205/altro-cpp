<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>AltroCpp: ALTRO Algorithm</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">AltroCpp
   &#160;<span id="projectnumber">0.3.4</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">ALTRO Algorithm </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This page aims to provide background information on the ALTRO algorithm and make the source code approachable and understandable. We will purposefully interweave the algorithm derivation together with the motivation behind the structure of the code.</p>
<p>We start with an overview of trajectory optimization, borrowing heavily from the original <a href="https://bjack205.github.io/papers/AL_iLQR_Tutorial.pdf">AL-iLQR tutorial</a> written by <a href="https://bjack205.github.io/">Brian Jackson</a>. We then derive the AL-iLQR algorithm, pointing frequently to where the equations are implemented in the source code.</p>
<h2><a class="anchor" id="autotoc_md13"></a>
Overview</h2>
<p><a class="el" href="classaltro_1_1Trajectory.html" title="Represents a state and control trajectory.">Trajectory</a> optimization is a powerful framework for controlling complicated robotic systems. The value of trajectory optimization lies primarily in its generality, allowing it to be applied to a very broad class of dynamical systems. Importantly, trajectory optimization can be applied to any type of dynamical system whose dynamics are Markovian, i.e. </p><p class="formulaDsp">
\begin{equation} \dot{x} = f(x,u) \end{equation}
</p>
<p> where \(\dot{x} \in \mathbb{R}^n \) is the time derivative of the state \( x \in \mathbb{R}^n \) and \(u \in \mathbb{R}^m \) are the controls.</p>
<p><a class="el" href="classaltro_1_1Trajectory.html" title="Represents a state and control trajectory.">Trajectory</a> optimization can be succinctly summarized by the underlying optimization problem being solved:</p>
<p class="formulaDsp">
\begin{align*} \underset{x(t), u(t)}{\text{minimize}} \quad &amp; \ell_T(x_T) + \int_{t=0}^{T} \ell(x(t), u(t)) dt \\ \text{subject to} \quad &amp; \dot{x} = f(x(t), u(t)) \\ &amp; g_{i}(x(t), u(t)) \leq_{K_i} 0 \\ &amp; h(x(t),u(t)) = 0 \end{align*}
</p>
<p> where \( x(t) \) and \( u(t) \) are the state and control trajectories from time \(0\) to \(T\). The dynamics constraint, which constrains derivatives of the optimization variables, is what sets trajectory optimization apart from other general nonlinear optimization problems. The field of trajectory optimization is dedicated to finding efficient ways to solve these nonlinear optimization problems.</p>
<p>The most common approach, and the one used by this software package, is to discretize the problem in time, dividing the time period of length \( T \) seconds into \( N \) segments, typically of equal length of \( h \) seconds. This results in \( N + 1 \) discretization points, also referred to as `&lsquo;knot&rsquo;' points, including the initial and final times. There exist many methods for approximating the integrals in the above optimization problem with discrete sums, as well as transforming the ordinary differential equation for the dynamics into a discrete difference equation of the form: </p><p class="formulaDsp">
\begin{equation} x_{k+1} = f(x_k, u_k, \Delta t). \end{equation}
</p>
<p> The accuracy and convergence of the algorithm is often directly related to the way in which the continuous-time problem is `&lsquo;transcribed&rsquo;' into a discrete optimization problem. This software package assumes that the user has already done the necessary work to arrive at a discrete optimization problem of the following form:</p>
<p class="formulaDsp">
\begin{align*} \underset{x_{0:N},u_{0:N-1}}{\text{minimize}} \quad &amp; \ell_N(x_N) + \sum_{k=0}^{N-1} \ell_k(x_k, u_k, dt) \\ \text{subject to} \quad &amp; x_{k+1} = f(x_k, u_k), &amp;&amp;\; k \in [0,N) \\ &amp; g_{k,i}(x_k,u_k) \leq_{K_i} 0, &amp;&amp;\; k \in [0,N] \\ &amp; h_k(x_k,u_k) = 0, &amp;&amp;\; k \in [0, N]. \end{align*}
</p>
<p>There exist many methods for solving problems of this form. These methods are typically divided into two categories: "indirect" and "direct" methods. Direct methods treat both the states and controls as decision variables and use general-purpose nonlinear programming (NLP) solvers, such as SNOPT or IPOPT. These methods typically use implicit integration schemes which are more numerically stable than their explicit counterparts. The most common method, direct collocation (DIRCOL), uses Hermite-Simpson integration to integrate both the cost and the dynamics, which is essentially a 3rd order implicit Runge-Kutta integrator for the states and first-order hold (i.e. linear interpolation) for the controls. These methods benefit directly from the robustness and generality of the NLP solvers on which they depend. However, direct methods also tend to be fairly slow and require dependencies on large optimization packages, many of which are closed-source and behind expensive paywalls.</p>
<p>Alternatively, indirect methods exploit the Markov structure in the trajectory optimization problem and often impose strict Markovianity across the entire problem, including the cost functions and constraints. The dynamics constraints are then implicitly enforced by simulating forward the system's dynamics. Differential Dynamic Programming (DDP) and iterative LQR (iLQR) are closely related indirect methods that solve the discrete trajectory optimization problem by breaking it into a sequence of smaller sub-problems. DDP methods improve on more naive "simple shooting" methods by incorporating a feedback policy during the forward simulation of the dynamics at each time step. Because of their strict enforcement of dynamic feasibility, it is often difficult to find a control sequence that produces a reasonable initialization for DDP methods. While they are fast and have a low memory footprint&mdash;making them amenable to embedded implementation&mdash;DDP methods have historically been considered less numerically robust and less well-suited to handling nonlinear state and input constraints. However, with recent advances in the underlying algorithms, several high-quality open-source DDP-based optimal control solvers have been released by the academic community, demonstrating impressive performance on a wide range of robotic systems (see <a href="https://github.com/leggedrobotics/ocs2">OCS2</a>, and <a href="https://github.com/loco-3d/crocoddyl">Crocoddyl</a>).</p>
<p>This package implements Augmented Lagrangian iLQR, the core algorithm behind the original ALTRO algorithm (see the <a href="https://roboticexplorationlab.org/papers/altro-iros.pdf">original paper</a> and <a href="https://github.com/RoboticExplorationLab/TrajectoryOptimization.jl">Julia implementation</a>). This algorithm is derived in the following sections following some background on the augmented Lagrangian method (ALM).</p>
<h2><a class="anchor" id="autotoc_md14"></a>
Notation</h2>
<p>Before we proceed with the mathematical derivation, we define a bit of useful mathematical notation.</p>
<p>Let </p><p class="formulaDsp">
\[ \nabla_x f(\cdot) \in \mathbb{R}^n \]
</p>
<p> be the partial derivative of \( f \) with respect to \( x \in \mathbb{R}^n \). Note that this is defined to be a column vector.</p>
<p>Likewise, let </p><p class="formulaDsp">
\[ \nabla_{xx}^2 f(\cdot) \in \mathbb{R}^{n \times n} \]
</p>
<p> if \( f(x) \in \mathbb{R} \).</p>
<p>If the output of the function \( f \) is a vector in \( \mathbb{R}^m \), the second derivative is a rank-3 tensor. To avoid dealing with the complexity that arises with these terms, we assume that the term we want is actually the Jacobian of a Jacobian-transpose-vector-product: </p><p class="formulaDsp">
\[ \nabla_x f(\cdot)^T b \]
</p>
<p> for some vector \( b \in \mathbb{R}^m \). We write this notationally as </p><p class="formulaDsp">
\[ \nabla_{xx}^2 f(\cdot, b) \in \mathbb{R}^{n \times n}. \]
</p>
<h2><a class="anchor" id="autotoc_md15"></a>
The Augmented Lagrangian Method</h2>
<p>The augmented Lagrangian Method is an optimization method for solving constrained optimization problems of the form</p>
<p class="formulaDsp">
\begin{align*} \underset{x}{\text{minimize}} \quad &amp; f(x) \\ \textrm{subject to} \quad &amp; g_i(x) \leq_{K_i} 0, \quad i = 0,\dots,n_K-1 \\ \end{align*}
</p>
<p> where</p><ul>
<li>\( f(x) \) is the objective, and</li>
<li>\( g_i(x) \) is a generalized equality constraint with respect to the cone \( K_i \).</li>
</ul>
<p>One of the easiest methods for solving constrained optimization problems is to move the constraints into the cost function and iteratively increase the penalty for either getting close to or actually violating the constraint. However, simple quadratic penalty methods only converge to the optimal answer as the penalty terms are increased to infinity, which is impractical to implement in a numerical optimization routine with limited-precision arithmetic. Augmented Lagrangian methods improve on penalty methods by maintaining estimates of the Lagrange multipliers associated with the constraints. This is accomplished by forming the augmented Lagrangian: </p><p class="formulaDsp">
\begin{equation} \mathcal{L}_\rho = f(x) + \sum_{i=0}^{n_K - 1} \lambda_i^T g_i(x) + \frac{1}{2 \rho} \left( || \Pi_{K_i^*} (\lambda_i - \rho g_i(x)) ||_2^2 - ||\lambda_i||_2^2 \right), \end{equation}
</p>
<p> where</p><ul>
<li>\( \lambda_i \) are the Lagrangian multipliers (dual variables) for constraint \( g_i \),</li>
<li>\( \rho \in \mathbb{R} \) is positive scalar penalty, and</li>
<li>\( \Pi_{K_i^*} \) is the projection operator onto the dual cone \( K_i^* \).</li>
</ul>
<p>This value is computed in <code><a class="el" href="classaltro_1_1augmented__lagrangian_1_1ALCost.html#a606d20356c1d36ecb99b30379d3f4d61" title="Evaluate the augmented Lagrangian cost.">augmented_lagrangian::ALCost::Evaluate</a></code>, which calls <code><a class="el" href="classaltro_1_1constraints_1_1ConstraintValues.html#ad562bdea1e02ee9d6ccb06b94967a527" title="Evaluate the augmented Lagrangian.">constraints::ConstraintValues::AugLag</a></code> to evaluate the elements of the sum in the equation above.</p>
<p>Currently, the following cones are implemented:</p><ul>
<li><code><a class="el" href="classaltro_1_1constraints_1_1ZeroCone.html" title="An Equality constraint (alias for ZeroCone)">constraints::ZeroCone</a></code> (an equality constraint)</li>
<li><code><a class="el" href="classaltro_1_1constraints_1_1NegativeOrthant.html" title="The space of all negative numbers, an alias for inequality constraints.">constraints::NegativeOrthant</a></code> (an inequality constraint)</li>
</ul>
<p>By treating equality constraints as projections on the the zero or null cone, we can treat all constraints identically as long as we have a well-defined projection operator for the corresponding dual cone. This leads to a clean, well-structured code with minimal branching logic to handle different types of constraints.</p>
<p>If we only have equality constraints, we can re-arrange the augmented Lagrangian equation into its more intuitive form that commonly appears in textbooks: </p><p class="formulaDsp">
\begin{equation} \mathcal{L}_\rho = f(x) - \lambda^T g(x) + \frac{\rho}{2} g(x)^T g(x). \end{equation}
</p>
<p> When writing the augmented Lagrangian in this form, we can easily see that the augmented Lagrangian is simply the normal Lagrangian with an additional quadratic penalty term. The simpler equation above can be derived from the original, more general form by using the fact that for equality constraints projection onto the dual cone is just the identity map, i.e. \( \Pi_{K^*}(x) = x \), and then expanding the quadratic terms and simplifying.</p>
<h3><a class="anchor" id="autotoc_md16"></a>
Dual Update</h3>
<p>Once we have formed the augmented Lagrangian, we solve the original problem by iteratively minimizing the augmented Lagrangian using an unconstrained nonlinear optimization method. Once we have found a local minima, we update the Lagrange multipliers with the following "dual update:" </p><p class="formulaDsp">
\begin{equation} \lambda_i = \Pi_{K_i^*} (\lambda_i - \rho g_i(x)) \end{equation}
</p>
<p> The motivation for the update can most easily be seen by rearranging the simpler form for equality constraints as follows: </p><p class="formulaDsp">
\begin{equation} \mathcal{L}_\rho = f(x) - \left(\lambda - \frac{\rho}{2} g(x) \right)^T g(x). \end{equation}
</p>
<p> where the term in the parentheses (which closely matches the dual update), looks just like Lagrange multiplier for the new, augmented problem.</p>
<p>The dual update is applied on a per-constraint basis in <code><a class="el" href="classaltro_1_1constraints_1_1ConstraintValues.html#a3c3a9bb2489fbd7539e32e5c5b4312a1" title="Update the dual variables.">constraints::ConstraintValues::UpdateDuals</a></code>, which is called via <code><a class="el" href="classaltro_1_1augmented__lagrangian_1_1AugmentedLagrangianiLQR.html#a5eea2af6743c451b0688b1b3f9789cdb" title="Update the dual variables for all of the constraints.">augmented_lagrangian::AugmentedLagrangianiLQR::UpdateDuals</a></code> calling <code><a class="el" href="classaltro_1_1augmented__lagrangian_1_1ALCost.html#a8924896ad514e04213e0b62b46dc50c1" title="Apply the dual update to all of the constraints.">augmented_lagrangian::ALCost::UpdateDuals</a></code> for each of the knot points.</p>
<h3><a class="anchor" id="autotoc_md17"></a>
Penalty Update</h3>
<p>After each constrained solve, the outer loop also has the option of updating the penalty parameter. According to the theory, superlinear convergence can be achieved as long as the penalty parameter is increased at a geometric rate, and the solution can be found if the penalty is above some theoretical, unknown critical penalty parameter \( \rho_\text{crit} \). In practice, the penalty is generally updated by a constant geometric factor: </p><p class="formulaDsp">
\[ \rho = \phi \rho \]
</p>
<p> where \( 2 \leq \phi \leq 10 \).</p>
<h2><a class="anchor" id="autotoc_md18"></a>
Augmented Lagrangian iLQR</h2>
<p>Now that we have a background on augmented Lagrangian, we can derive AL-DDP / AL-iLQR. The difference between these two is minor, and will be made clear during the derivation.</p>
<p>The key idea of DDP is that at each iteration, all nonlinear constraints and objectives are approximated using first or second-order Taylor series expansions so that the approximate functions, now operating on deviations about the nominal trajectory, can be solved using discrete LQR. The locally-optimal feedback policy is omputed during the "backward pass," so named since LQR starts at the end of the trajectory and moves backward in time along the trajectory. This feedback policy is then used to simulate the system forward during the "forward
pass," which modifies the policy as needed in order to ensure we make progress towards minimizing our actual nonlinear objective.</p>
<p>To handle constraints, we simply "augment" the cost function with the multiplier and penalty terms ofthe augmented Lagrangian, treating the dual variables \( \lambda \) and the penalty term \( \rho \) as constants. After a locally optimal solution is found, any constraint violation is used to update the dual variables, the penalty is updated, and the process is repeated until the constraint violations get sufficiently small.</p>
<h3><a class="anchor" id="autotoc_md19"></a>
The Backward Pass</h3>
<p>We first form a new, unconstrained version of our original discrete optimization problem using the augmented Lagrangian:</p>
<p class="formulaDsp">
\begin{align*} \underset{x_{0:N},u_{0:N-1}}{\text{minimize}} \quad &amp; \mathcal{L}_N(x_N) + \sum_{k=0}^{N-1} \mathcal{L}_k(x_k, u_k, dt) \\ \text{subject to} \quad &amp; x_{k+1} = f(x_k, u_k), &amp;&amp;\; k \in [0,N) \\ \end{align*}
</p>
<p> where </p><p class="formulaDsp">
\[ \mathcal{L}_k = \ell_k(x_k, u_k) + \frac{1}{2\rho} \sum_{i=0}^{n_{K,k}} || \Pi_{K_i^*}\left(\lambda_{k,i} - \rho g_{k,i}(x_i, u_i)) ||_2^2 - || \lambda_{k,i} ||_2^2 \right). \]
</p>
<p> Here we've abused notation slightly, incorporating the equality constraints into the conic constraints following the same approach used in the background on augmented Lagrangian. Here each time step has \( n_{K,k} \) separate constraint functions, each of which may be with respect to a different cone.</p>
<p>This conversion happens transparently to the user in <code><a class="el" href="namespacealtro_1_1augmented__lagrangian.html#a442df3eb13e45d62b7983c314802b5c6" title="Build the augmented Lagrangian trajectory optimization problem.">augmented_lagrangian::BuildAugLagProblem</a></code>, where the user-specified problem is converted into a problem with zero constraints and <code><a class="el" href="classaltro_1_1augmented__lagrangian_1_1ALCost.html" title="An augmented Lagrangian cost function, which adds the linear and quadratic penalty costs to an existi...">augmented_lagrangian::ALCost</a></code> cost functions, which contain the original objective along with vectors of <code><a class="el" href="classaltro_1_1constraints_1_1ConstraintValues.html" title="A constraint that also allocates memory for constraint values, Jacobians, dual variables,...">constraints::ConstraintValues</a></code>, one for each constraint at that knotpoint in the original problem.</p>
<p>We start our derivation by defining the cost-to-go function: </p><p class="formulaDsp">
\begin{align} V_i(x) = \underset{u_{i:N-1}}{\text{minimize}} &amp;&amp;&amp; \mathcal{L}_N + \sum_{k=i}^{N-1} \mathcal{L}_k(x_k, u_k) \\ \text{subject to} &amp;&amp;&amp; x_{k+1} = f_k(x_k, u_k) \end{align}
</p>
<p> which intuitively is just the optimal cost can achieve from state \( x_k \) if we apply the optimal controls, which is naturally subject to the discrete dynamics \( x_{k+1} = f_k(x_k, u_k) \). From the Bellman equation and principle of optimality we can re-define the cost-to-go function in a more convenience recursive form: </p><p class="formulaDsp">
\[ V_k(x) = \min_{u} \{ \mathcal{L}_k(x, u) + V_{k+1}(f_k(x, u)) \}. \]
</p>
<p> For convenience, we define the operand of the minimization to be the action-value function \( Q_k(x, u) \): </p><p class="formulaDsp">
\[ V_k(x) = \min_{u} Q(x, u). \]
</p>
<p>In general, the true cost-to-go function for a nonlinear trajectory optimization problem is an arbitrarily complex nonlinear function. To make the computation tractable, we are going to find a local approximation of the true cost-to-go in the space around our current nominal trajectory. In line with LQR, we assume that the cost-to-go is locally quadratic: </p><p class="formulaDsp">
\[ V_k(x) \approx V_k(x_k) + p_k^T \delta x_k + \frac{1}{2} \delta x_k^T P_k \delta x_k, \]
</p>
<p> where \( x_k \) is from our nominal trajectory, and \( \delta x_k = x - x_k \).</p>
<p>We compute the cost-to-go recursively, starting at the tail where we know that </p><p class="formulaDsp">
\begin{align} P_N &amp;= \nabla_{xx}^2 \mathcal{L}_N(\bar{x}_N) \\ p_N &amp;= \nabla_x \mathcal{L}_k(\bar{x}_N) \end{align}
</p>
<p> since there is no control over which to optimize. This is computed in <code><a class="el" href="classaltro_1_1ilqr_1_1KnotPointFunctions.html#acac77f225f7dec7eadd193f3d2b61139" title="Calculate the terminal cost-to-go, or the cost-to-go at the last knot point.">ilqr::KnotPointFunctions::CalcTerminalCostToGo</a></code>.</p>
<p>To find \( V_k(x) \) we need to optimize \( Q_k(x, u) \) with respect to \( u_k \), which we can do by taking a second-order approximation of \( Q_k(x, u) \): </p><p class="formulaDsp">
\[ Q_k(x) \approx Q_k(\bar{x}_k) + \begin{bmatrix} \delta x_k \\ \delta u_k \end{bmatrix}^T \begin{bmatrix} Q_{xx} &amp; Q_{xu} \\ Q_{ux} &amp; Q_{uu} \end{bmatrix} \begin{bmatrix} \delta x_k \\ \delta u_k \end{bmatrix} + \begin{bmatrix} Q_x \\ Q_u \end{bmatrix}^T \begin{bmatrix} \delta x_k \\ \delta u_k \end{bmatrix} \]
</p>
<p> where </p><p class="formulaDsp">
\begin{align} Q_{xx} &amp;= \nabla_{xx}^2 \mathcal{L}_k(\cdot) + \nabla_x f(\cdot)^T P_{k+1} \nabla_x f(\cdot) + \nabla_{xx}^2 f(\cdot, p_{k+1}) \\ Q_{xu} &amp;= \nabla_{xu}^2 \mathcal{L}_k(\cdot) + \nabla_x f(\cdot)^T P_{k+1} \nabla_u f(\cdot) + \nabla_{xu}^2 f(\cdot, p_{k+1}) \\ Q_{uu} &amp;= \nabla_{uu}^2 \mathcal{L}_k(\cdot) + \nabla_u f(\cdot)^T P_{k+1} \nabla_u f(\cdot) + \nabla_{uu}^2 f(\cdot, p_{k+1}) \\ Q_{x} &amp;= \nabla_{x} \mathcal{L}_k(\cdot) + \nabla_x f(\cdot)^T p_{k+1} \\ Q_{u} &amp;= \nabla_{u} \mathcal{L}_k(\cdot) + \nabla_u f(\cdot)^T p_{k+1}. \end{align}
</p>
<p> Here the first term comes from the expansion of the augmented Lagrangian, which is given below. The second terms come from the expansion of the \( V_{k+1}(f_k(x_k, u_k)) \) term, where we use the values of \( P_k \) and \( p_k \) from the next knot point (which we've already computed). The third term in the second-order terms is a tensor term that comes from the second-order expansion of the dynamics. Leaving this term off gives the Gauss Newton step instead of the full Newton step, which is the only difference between DDP and iLQR. While leaving this term off decreases the convergence rate and therefore the solver has to take more iterations, each iteration is often much cheaper computationally so iLQR often outperforms DDP in computation time. Another advantage of using iLQR is that as long as the Hessian of the cost function is positive-definite, these second-order terms will also be positive-definite, which is not the case for DDP, which therefore needs have much more advanced safeguards for ensuring the system is positive definite before solving for the optimal control. For these reasons, the library currently only implements iLQR, but may implement DDP in future releases. The expansion of the action-value function is computed in <code><a class="el" href="classaltro_1_1ilqr_1_1KnotPointFunctions.html#af4a5c33d2a8ba16ca8543d03d5b06cc8" title="Calculate the action-value expansion given the quadratic approximation of the cost-to-go at the next ...">ilqr::KnotPointFunctions::CalcActionValueExpansion</a></code>.</p>
<p>The expansion of the augmented Lagrangian slighly involved, thanks to the projection operator. The full expansions are: </p><p class="formulaDsp">
\begin{align} \nabla_{xx} \mathcal{L}_k &amp;= \nabla_{xx} \ell_k(\cdot) - \sum_{i=0}^{n_{K,k}} - \rho \nabla_{x} g_{k,i}(\cdot)^T \nabla \Pi_{K_i^*}(\tilde{\lambda}_{k,i})^T \Pi_{K_i^*}(\tilde{\lambda}_{k,i}) \nabla_{x} g_{k,i}(\cdot) \\ &amp;\quad + \nabla_{xx}^2 g_{k,i}\left(\cdot, \nabla \Pi_{K_i^*}(\tilde{\lambda}_{k,i})^T \Pi_{K_i^*}(\tilde{\lambda}_{k,i}) \right) - \rho \nabla_{x} g_{k,i}(\cdot)^T \nabla^2 \Pi_{K_i^*}\left(\tilde{\lambda}_{k,i}, \Pi_{K_i^*}(\tilde{\lambda}_{k,i}) \right) \nabla_{x} g_{k,i}(\cdot) \\ \nabla_{xu} \mathcal{L}_k &amp;= \nabla_{xu} \ell_k(\cdot) - \sum_{i=0}^{n_{K,k}} - \rho \nabla_{x} g_{k,i}(\cdot)^T \nabla \Pi_{K_i^*}(\tilde{\lambda}_{k,i})^T \Pi_{K_i^*}(\tilde{\lambda}_{k,i}) \nabla_{u} g_{k,i}(\cdot) \\ &amp;\quad + \nabla_{xu}^2 g_{k,i}\left(\cdot, \nabla \Pi_{K_i^*}(\tilde{\lambda}_{k,i})^T \Pi_{K_i^*}(\tilde{\lambda}_{k,i}) \right) - \rho \nabla_{x} g_{k,i}(\cdot)^T \nabla^2 \Pi_{K_i^*}\left(\tilde{\lambda}_{k,i}, \Pi_{K_i^*}(\tilde{\lambda}_{k,i}) \right) \nabla_{u} g_{k,i}(\cdot) \\ \nabla_{uu} \mathcal{L}_k &amp;= \nabla_{uu} \ell_k(\cdot) - \sum_{i=0}^{n_{K,k}} - \rho \nabla_{u} g_{k,i}(\cdot)^T \nabla \Pi_{K_i^*}(\tilde{\lambda}_{k,i})^T \Pi_{K_i^*}(\tilde{\lambda}_{k,i}) \nabla_{u} g_{k,i}(\cdot) \\ &amp;\quad + \nabla_{uu}^2 g_{k,i}\left(\cdot, \nabla \Pi_{K_i^*}(\tilde{\lambda}_{k,i})^T \Pi_{K_i^*}(\tilde{\lambda}_{k,i}) \right) - \rho \nabla_{u} g_{k,i}(\cdot)^T \nabla^2 \Pi_{K_i^*}\left(\tilde{\lambda}_{k,i}, \Pi_{K_i^*}(\tilde{\lambda}_{k,i}) \right) \nabla_{u} g_{k,i}(\cdot) \\ \nabla_{x} \mathcal{L}_k &amp;= \nabla_{x} \ell_k(\cdot) - \sum_{i=0}^{n_{K,k}} \nabla_{x} g_{k,i}(\cdot)^T \nabla \Pi_{K_i^*}(\tilde{\lambda}_{k,i})^T \Pi_{K_i^*}(\tilde{\lambda}_{k,i}) \\ \nabla_{u} \mathcal{L}_k(\cdot) &amp;= \nabla_{u} \ell_k(\cdot) - \sum_{i=0}^{n_{K,k}} \nabla_{u} g_{k,i}(\cdot)^T \nabla \Pi_{K_i^*}(\tilde{\lambda}_{k,i})^T \Pi_{K_i^*}(\tilde{\lambda}_{k,i}) \\ \end{align}
</p>
<p> As with the expansion of the action-value expansion, we drop the tensor terms (the terms in the second row of each of the second-order terms) for computational efficiency. It's also worth noting that for inequality and equality constraints, the last term in these expansions is always zero since the dual-cone projection operators are linear.</p>
<p>With our approximation of the action-value expansion, we can now solve for the optimal value of \( \delta u \) by taking the derivative of \( \delta Q_k \) with respect to \( \delta u \), setting it to zero, and solving for \( \delta u \). This gives our locally-optimal control policy: </p><p class="formulaDsp">
\[ \delta u_k^* = -(Q_{uu} + \mu I)^{-1}(Q_{ux} \delta x_k + Q_u) = K_k \delta x_k + d_k \]
</p>
<p> where \( \mu \in \mathbb{R} \) is a positive scalar regularizer that is used to keep the system positive-definite. The regularization is added to the expansion in <code><a class="el" href="classaltro_1_1ilqr_1_1KnotPointFunctions.html#a6f9dcd4ceb1abd2fe97fd2bea67af3b1" title="Add regularization to the action-value expansion prior to solving for the optimal feedback policy.">ilqr::KnotPointFunctions::RegularizeActionValue</a></code>, and these values are then used to compute the gains in <code><a class="el" href="classaltro_1_1ilqr_1_1KnotPointFunctions.html#a514b9004a74567c15fe20d306931fc5d" title="Calculate the feedback and feedforward gains by inverting the Hessian of the action-value expansion w...">ilqr::KnotPointFunctions::CalcGains</a></code>. The regularization is automatically updated during the solve. If the Cholesky factorization of the regularized version of \( Q_{uu} \) fails, the regularization is updated and the backward pass restarts from the beginning. If the backward pass succeeds, the regularization is decreased. The regularization can also be updated if the forward pass fails to find a solution that make sufficient progress in decreasing the cost (more on that later). The regularization uses a simple first-order system to adaptively add and decrease the regularization. This is handled via <code><a class="el" href="classaltro_1_1ilqr_1_1iLQR.html#a68940083145760db8ce97986fc8036e5" title="Increase the regularization, steering the steps closer towards gradient descent (more robust,...">ilqr::iLQR::IncreaseRegularization</a></code> and <code><a class="el" href="classaltro_1_1ilqr_1_1iLQR.html#a0c09baaa6d433aa7947deea983ce828f" title="Decrease the regularization term.">ilqr::iLQR::DecreaseRegularization</a></code>. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.17
</small></address>
</body>
</html>
